{"id": "paper_1", "title": "Attention Is All You Need", "abstract": "We propose the Transformer, a novel neural network architecture based entirely on self-attention mechanisms, dispensing with recurrence and convolutions entirely.", "year": 2017, "citations": 45000, "venue": "NeurIPS", "url": "https://arxiv.org/abs/1706.03762"}
{"id": "paper_2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce BERT, a method for pre-training language representations which obtains state-of-the-art results on a wide array of NLP tasks.", "year": 2019, "citations": 65000, "venue": "NAACL", "url": "https://arxiv.org/abs/1810.04805"}
{"id": "paper_3", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "abstract": "We present RAG, a model that combines pre-trained parametric and non-parametric memory for open-domain question answering and knowledge-intensive tasks.", "year": 2020, "citations": 12000, "venue": "NeurIPS", "url": "https://arxiv.org/abs/2005.11401"}
{"id": "paper_4", "title": "LoRA: Low-Rank Adaptation of Large Language Models", "abstract": "LoRA is an efficient fine-tuning method for large language models using low-rank adaptation of weight matrices, reducing trainable parameters and memory.", "year": 2022, "citations": 8000, "venue": "ICLR", "url": "https://arxiv.org/abs/2106.09685"}
{"id": "paper_5", "title": "RAG Retriever Optimization for Open-domain QA", "abstract": "We explore retriever architectures that improve the retrieval component in retrieval-augmented generation (RAG) systems using hybrid dense-sparse approaches.", "year": 2023, "citations": 1200, "venue": "ACL", "url": "https://arxiv.org/abs/2303.12345"}
